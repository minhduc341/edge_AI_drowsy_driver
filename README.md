# Drowsiness Detection using MobileNetV2 on STM32H750

Real-time drowsiness detection for drivers using an edge AI model deployed directly on the STM32H750 microcontroller. The system captures live grayscale images from a camera, processes them using a lightweight MobileNetV2-based model, and displays the result on a TFT LCD screen.

---

## ğŸ“Œ Project Overview

This project implements a lightweight object detection model to detect whether a driver is drowsy or not. It leverages a reduced MobileNetV2 backbone trained and converted for real-time inference on STM32H750 using STM32Cube.AI. The model takes grayscale input from a camera and displays predictions on an LCD screen.

Target application: embedded, on-device drowsiness monitoring for smart vehicles or helmets.

---

## ğŸ¯ Objectives

- Detect drowsy vs. non-drowsy states in real-time
- Use grayscale camera input (128x128 resolution)
- Deploy deep learning model directly on STM32H750 (no cloud/edge server)
- Display results on 0.96â€ TFT LCD screen
- Run with minimal power and memory usage

---

## ğŸ§  AI Model Details

- Backbone: MobileNetV2 (width multiplier Î± = 0.1)
- Truncated: first 5 blocks used (stride = 8)
- Input: 128Ã—128 grayscale image â†’ repeated to 3 channels
- Output: feature map size (3 Ã— 16 Ã— 16)
  - 0 = background
  - 1 = drowsy
  - 2 = non-drowsy
- Head:
  - Depthwise Conv + Pointwise Conv
  - Interpolated to 16x16 output grid
- Activation: ReLU
- Trained using PyTorch and exported via X-Cube.AI

---

## ğŸ›  Hardware & Software Requirements

ğŸ§© Hardware:

- STM32H750 board (tested on STM32H750VBT6)
- OV2640 camera (connected via DCMI)
- 0.96â€ TFT LCD (SPI interface)
- Optional: External QSPI Flash (for model storage)

ğŸ’» Software:

- STM32CubeIDE
- STM32Cube.AI (X-Cube-AI)
- Python 3.10+
- PyTorch
- OpenCV (for preprocessing and augmentation)
- Jupyter Notebook (optional for training scripts)

---

## ğŸ“¦ Project Structure

```
.
â”œâ”€â”€ model/ # AI model files
â”‚ â”œâ”€â”€ mobilenetv2_fomo.pt
â”‚ â”œâ”€â”€ model.onnx
â”‚ â””â”€â”€ ai_model.c/.h (from X-Cube-AI)
â”œâ”€â”€ data/ # Training and validation data
â”‚ â”œâ”€â”€ train/
â”‚ â”œâ”€â”€ test/
â”‚ â””â”€â”€ _annotations.csv
â”œâ”€â”€ stm32_project/ # STM32CubeIDE project
â”‚ â”œâ”€â”€ Core/
â”‚ â”œâ”€â”€ Inc/
â”‚ â”œâ”€â”€ Src/
â”‚ â””â”€â”€ main.c
â”œâ”€â”€ notebooks/ # Training & conversion notebooks
â”‚ â”œâ”€â”€ train_model.ipynb
â”‚ â””â”€â”€ convert_model_xcubeai.ipynb
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

yaml
Always show details

Copy

---


## ğŸš€ Getting Started

1. Clone the repository
2. Train the model (optional, model is provided)
3. Convert model to C array using X-Cube-AI
4. Open stm32_project in STM32CubeIDE
5. Flash firmware to STM32H750
6. Connect camera + LCD and run

---


## ğŸ“ˆ Performance

- Accuracy on test set: ~80%
- Model complexity: 24.98M MACC
- Flash usage: ~30 KB (model only)
- RAM usage: ~407 KB
- Real-time FPS on STM32H750: ~15 FPS
- Limitations: Real-time predictions on STM32H750 are less accurate than offline results due to quantization & lighting variation.

---


## ğŸ”§ Re-training & Conversion

- Training data format: 128Ã—128 grayscale images
- Labels: stored in _annotations.csv (bounding boxes + class)
- Augmentation: basic shift (Â±2 px), horizontal flip
- Conversion path:
  â†’ PyTorch (.pt) â†’ ONNX â†’ X-Cube-AI (.c/.h)  
- Run convert_model_xcubeai.ipynb to export ONNX model

---


## ğŸ“‹ Inference Logic

- Image divided into 16Ã—16 grid
- For each grid cell:
  - If overlaps with a bounding box:
    â†’ label = 1 (drowsy) or 2 (non-drowsy)
  - Else: label = 0
- Output: 3x16x16 probability map

---


## ğŸ“ License

This project is licensed under the MIT License.  
Please refer to LICENSE file for details.

Note: The dataset used is derived from synthetic driving scenes in Roblox and is intended for educational/research use only.

---


## ğŸ¤ Contributions & Contact

Pull requests are welcome.  
For major changes, please open an issue first to discuss.

Author: [Your Name]  
Email: your@email.com  
GitHub: https://github.com/your_username

---


ğŸ“· Demo Image

You can add a real LCD output image in images/ folder and show in README:

![LCD Output Demo](images/demo_lcd.jpg)
"""

with open("/mnt/data/README.md", "w") as f:
    f.write(readme_content)

"/mnt/data/README.md"
