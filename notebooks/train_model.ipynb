{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d95208-8ef1-46f4-9914-060ad6975d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Đường dẫn đến DataSet\n",
    "root_path = r\"dataset\"\n",
    "\n",
    "cls_lst = ['Drowsy', 'Non Drowsy']\n",
    "\n",
    "# Hàm để đọc dữ liệu từ thư mục\n",
    "def load_data_with_bbox(data_dir, target_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Đọc dữ liệu và resize ảnh về kích thước target_size, đồng thời điều chỉnh lại bounding box.\n",
    "    Ảnh được giữ ở định dạng grayscale (1 kênh).\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Đường dẫn đến thư mục chứa dữ liệu.\n",
    "        target_size (tuple): Kích thước mong muốn của ảnh (width, height).\n",
    "\n",
    "    Returns:\n",
    "        images (list): Danh sách ảnh grayscale đã resize (numpy array, shape: target_size + (1,)).\n",
    "        bboxes_with_classes (list): Danh sách [[xmin, ymin, width, height], class].\n",
    "    \"\"\"\n",
    "    # Đọc file _annotations.csv\n",
    "    annotations = pd.read_csv(os.path.join(data_dir, \"_annotations.csv\"))\n",
    "\n",
    "    # Tách dữ liệu ảnh và nhãn\n",
    "    images = []  # Danh sách chứa ảnh grayscale đã resize\n",
    "    bboxes_with_classes = []  # Danh sách chứa [[xmin, ymin, width, height], class]\n",
    "\n",
    "    for _, row in annotations.iterrows():\n",
    "        img_path = os.path.join(data_dir, str(row['filename']))\n",
    "        if os.path.exists(img_path):\n",
    "            # Mở ảnh và lấy kích thước ban đầu\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            original_width, original_height = img.size\n",
    "\n",
    "            # Resize ảnh về kích thước target_size\n",
    "            img_resized = img.resize(target_size)\n",
    "\n",
    "            # Chuyển đổi sang ảnh xám (grayscale)\n",
    "            img_gray = img_resized.convert(\"L\")\n",
    "\n",
    "            # Chuyển thành numpy array và thêm chiều kênh (shape: 128, 128, 1)\n",
    "            img_array = np.array(img_gray)\n",
    "            img_array = np.expand_dims(img_array, axis=-1)\n",
    "\n",
    "            # Chuẩn hóa dữ liệu về khoảng [0, 1]\n",
    "            img_array = img_array / 255.0\n",
    "\n",
    "            images.append(img_array)\n",
    "\n",
    "            # Tính toán tỷ lệ thay đổi kích thước\n",
    "            scale_x = target_size[0] / original_width\n",
    "            scale_y = target_size[1] / original_height\n",
    "\n",
    "            # Điều chỉnh bounding box cho phù hợp với ảnh đã resize\n",
    "            xmin = row['xmin'] * scale_x\n",
    "            ymin = row['ymin'] * scale_y\n",
    "            xmax = row['xmax'] * scale_x\n",
    "            ymax = row['ymax'] * scale_y\n",
    "\n",
    "            # Tạo bounding box với nhãn lớp\n",
    "            bbox = [xmin, ymin, xmax - xmin, ymax - ymin]\n",
    "            class_id = 0 if row['class'] == \"Drowsy\" else 1  # 0: Drowsy, 1: Non Drowsy\n",
    "            bboxes_with_classes.append([bbox, class_id])\n",
    "\n",
    "    return images, bboxes_with_classes\n",
    "\n",
    "# Đọc dữ liệu\n",
    "X_train, y_train = load_data_with_bbox(os.path.join(root_path, \"train\"))\n",
    "X_test, y_test = load_data_with_bbox(os.path.join(root_path, \"test\"))\n",
    "X_val, y_val = load_data_with_bbox(os.path.join(root_path, \"valid\"))\n",
    "\n",
    "# Chuyển danh sách thành numpy array\n",
    "X_train = np.array(X_train)  # Shape: (n_train, 128, 128, 1)\n",
    "X_test = np.array(X_test)    # Shape: (n_test, 128, 128, 1)\n",
    "X_val = np.array(X_val)      # Shape: (n_val, 128, 128, 1)\n",
    "\n",
    "# Tách bounding box và nhãn lớp từ y\n",
    "y_train_boxes = np.array([item[0] for item in y_train])  # Shape: (n_train, 4)\n",
    "y_train_labels = np.array([item[1] for item in y_train]) # Shape: (n_train,)\n",
    "y_test_boxes = np.array([item[0] for item in y_test])    # Shape: (n_test, 4)\n",
    "y_test_labels = np.array([item[1] for item in y_test])   # Shape: (n_test,)\n",
    "y_val_boxes = np.array([item[0] for item in y_val])      # Shape: (n_val, 4)\n",
    "y_val_labels = np.array([item[1] for item in y_val])     # Shape: (n_val,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ebc36-2cfd-4edc-bc2e-73dac3e4ac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra dữ liệu\n",
    "print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}, Validation samples: {len(X_val)}\")\n",
    "print(f\"Example resized image shape (train): {X_train[0].shape}\")  # Kích thước ảnh đầu tiên\n",
    "print(f\"Example bbox with class (train): {y_train[0]}\")  # Bounding box và lớp của ảnh đầu tiên"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b876d9f7-1348-4616-859f-431ab4430d35",
   "metadata": {},
   "source": [
    "Hàm chuyển bbox (x_min, y_min, w, h) thành (x_center, y_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe0267b-beff-43d0-ae5f-e3a546cf10ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Hàm chuyển đổi bbox thành điểm trung tâm\n",
    "def bbox_to_center(bbox):\n",
    "  # bbox: [x_min, y_min, w, h]\n",
    "  x_center = bbox[0] + bbox[2]/2\n",
    "  y_center = bbox[1] + bbox[3]/2\n",
    "  return int(x_center), int(y_center)\n",
    "\n",
    "\n",
    "def bbox_to_segmentation(size_wh, bboxes, grid_size=16, num_classes=2):\n",
    "  \"\"\"\n",
    "  Convert bounding box annotations to segmentation maps for training.\n",
    "  Mỗi ô trong lưới có giá trị = 1 nếu nó giao với bbox, và 0 nếu không.\n",
    "  \"\"\"\n",
    "  h, w, _ = size_wh  # Sửa cách unpack cho ảnh grayscale (128, 128, 1)\n",
    "  seg_map = torch.zeros((num_classes + 1, grid_size, grid_size), dtype=torch.float32)  # Khởi tạo segmentation map\n",
    "\n",
    "  seg_map[0, :, :] = 1.0  # Background: mặc định là 1.0 ở tất cả các ô\n",
    "\n",
    "  # Tính toán kích thước mỗi cell (ô lưới)\n",
    "  cell_h, cell_w = h / grid_size, w / grid_size\n",
    "\n",
    "  # Duyệt qua tất cả các bounding box trong bboxes\n",
    "  # bboxes: [[xmin, ymin, width, height], class_id]\n",
    "  bbox = bboxes[0]  # Bounding box: [xmin, ymin, width, height]\n",
    "  class_id = bboxes[1]  # Lớp đối tượng (class_id)\n",
    "  #print(bbox)\n",
    "  x_min, y_min, width, height = bbox\n",
    "  x_max = x_min + width\n",
    "  y_max = y_min + height\n",
    "\n",
    "  # Chuyển đổi bbox thành các chỉ số ô lưới (grid cells)\n",
    "  grid_x_min = max(0, int(x_min // cell_w))\n",
    "  grid_y_min = max(0, int(y_min // cell_h))\n",
    "  grid_x_max = min(grid_size - 1, int(x_max // cell_w))\n",
    "  grid_y_max = min(grid_size - 1, int(y_max // cell_h))\n",
    "\n",
    "  # Cập nhật segmentation map cho tất cả các ô lưới có giao nhau với bbox\n",
    "  for grid_x in range(grid_x_min, grid_x_max + 1):\n",
    "    for grid_y in range(grid_y_min, grid_y_max + 1):\n",
    "      if 0 <= grid_x < grid_size and 0 <= grid_y < grid_size:\n",
    "        seg_map[class_id + 1, grid_x, grid_y] = 1.0  # Đánh dấu lớp đối tượng\n",
    "        seg_map[0, grid_x, grid_y] = 0.0  # Đánh dấu không phải background\n",
    "\n",
    "  return seg_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01eec33-4e02-4f41-be37-19b910c78e2c",
   "metadata": {},
   "source": [
    "Hàm chuyển đổi seg_map thành center và hàm vẽ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c9d4a4-bac8-4fbb-bfe6-4a37355fe5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def export_center(predicted, image_wh):\n",
    "  #x, y = torch.where(predicted == 1)\n",
    "  x, y = torch.where(torch.isin(predicted, torch.tensor([1, 2])))\n",
    "  h, w, _ = image_wh\n",
    "  print(image.size)\n",
    "  cell_h, cell_w = h / 16, w / 16\n",
    "\n",
    "  center_lst = []\n",
    "\n",
    "  for i in range(len(x)):\n",
    "    center_x = int((x[i] + 0.5) * cell_w)\n",
    "    center_y = int((y[i] + 0.5) * cell_h)\n",
    "    center_lst.append([center_x, center_y])\n",
    "  return center_lst\n",
    "\n",
    "def draw_center(image, center_lst):\n",
    "  image_pil = Image.fromarray((image.squeeze() * 255).astype(np.uint8), mode='L')\n",
    "  image_draw = image_pil.copy()\n",
    "  draw = ImageDraw.Draw(image_draw)\n",
    "  color = 255  # Màu trắng cho ảnh grayscale\n",
    "  radius = 0.5  # Tăng radius để dễ nhìn hơn\n",
    "  for center_x, center_y in center_lst:\n",
    "    draw.ellipse((center_x - radius, center_y - radius, center_x + radius, center_y + radius), fill=color)\n",
    "  return image_draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeefbb7d-b014-4497-82e4-ad7d517bbce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra\n",
    "import random\n",
    "idx = random.randint(0, len(X_train) - 1)\n",
    "print(\"Index:\", idx)\n",
    "\n",
    "image = X_train[idx]\n",
    "y = bbox_to_segmentation(image.shape, y_train[idx], 16, num_classes=2).unsqueeze(0)\n",
    "_, y_ = torch.max(y, 1)\n",
    "\n",
    "print(\"Segmentation map (max):\", y_)\n",
    "\n",
    "# Hiển thị ảnh với trung tâm\n",
    "plt.imshow(draw_center(image, export_center(y_[0], image.shape)), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e1296f-a2fe-4081-a8da-afbabf93ea06",
   "metadata": {},
   "source": [
    "Class Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deb8fba-e321-41fc-9ec6-c1723c5155be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class DrowsinessDetectionFOMODataset(Dataset):\n",
    "  def __init__(self, images, bboxes, grid_size=16, num_classes=2, transform=None):\n",
    "    self.images = images  # Dữ liệu đầu vào\n",
    "    self.bboxes = bboxes  # phân loại tương ứng\n",
    "    self.num_classes = num_classes\n",
    "    self.transform = transform\n",
    "    self.grid_size = grid_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.images)  # Số lượng mẫu trong dataset\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # Trả về một cặp (dữ liệu, nhãn) cho chỉ số idx\n",
    "    # waveform = np.load(self.data[idx])\n",
    "    image = self.images[idx]\n",
    "    bboxes = self.bboxes[idx]\n",
    "\n",
    "    # Tạo segmentation map\n",
    "    target = bbox_to_segmentation(image.shape, bboxes, self.grid_size, num_classes=self.num_classes)\n",
    "\n",
    "    # Chuyển numpy array thành PIL Image (grayscale)\n",
    "    image_pil = Image.fromarray((image.squeeze() * 255).astype(np.uint8), mode='L')\n",
    "\n",
    "    # Áp dụng transform nếu có\n",
    "    if self.transform:\n",
    "      image_pil = self.transform(image_pil)  # Transform trả về tensor\n",
    "      return image_pil, target  # Trả về trực tiếp tensor từ transform\n",
    "\n",
    "    # Nếu không có transform, chuyển thành tensor thủ công\n",
    "    image_tensor = torch.tensor(np.array(image_pil), dtype=torch.float32).unsqueeze(0) / 255.0  # Shape: (1, 128, 128)\n",
    "    return image_tensor, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809bbd0-18fa-4b85-a5ea-c460e7b1c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),   # Chuyển PIL Image thành tensor (shape: 1, 128, 128)\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Chuẩn hóa cho ảnh grayscale\n",
    "])\n",
    "\n",
    "train_dataset = DrowsinessDetectionFOMODataset(X_train, y_train, grid_size=16, transform=transform)\n",
    "val_dataset = DrowsinessDetectionFOMODataset(X_val, y_val, grid_size=16, transform=transform)\n",
    "test_dataset = DrowsinessDetectionFOMODataset(X_test, y_test, grid_size=16, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c52f7-23a5-47d4-81e2-cb4b3d319f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Tạo DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "print(\"Number of batches in train_loader:\", len(train_loader))\n",
    "\n",
    "# Lấy batch đầu tiên và kiểm tra\n",
    "for images, targets in train_loader:\n",
    "    # Chuyển dữ liệu sang device\n",
    "    images = images.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    print(\"Images shape:\", images.shape)\n",
    "    print(\"Targets shape:\", targets.shape)\n",
    "    print(\"Targets (segmentation map):\")\n",
    "    print(targets)\n",
    "    print(\"Unique values in targets:\", torch.unique(targets))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e9818f-5382-4706-9c7f-d445393e3e35",
   "metadata": {},
   "source": [
    "Tạo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955a750-5d6a-4084-bdf5-64a72741ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d61ed-2d54-4d39-9195-fdf089768269",
   "metadata": {},
   "source": [
    "Cấu hình Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f45eda-5e29-47d3-b77e-ca889cfe8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(r\"notebook\\mobilenetv2.pytorch\")\n",
    "from models.imagenet import mobilenetv2\n",
    "\n",
    "class FOMOWithMobileNetV2(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_size=(128, 128), alpha=0.1, num_classes=3, pretrain_path=''):\n",
    "        super(FOMOWithMobileNetV2, self).__init__()\n",
    "\n",
    "        # Load MobileNetV2 backbone với alpha=0.1\n",
    "        backbone = mobilenetv2(width_mult=alpha)\n",
    "        if os.path.exists(pretrain_path):\n",
    "            backbone.load_state_dict(torch.load(pretrain_path, map_location=torch.device('cpu')), strict=False)\n",
    "\n",
    "        # Truncate MobileNetV2 đến block 5 (stride 8)\n",
    "        self.feature_extractor = nn.Sequential(*list(backbone.features)[:5])  \n",
    "\n",
    "        # Lấy số kênh đầu ra của feature_extractor\n",
    "        sample_input = torch.randn(1, 3, 128, 128)  # Test đầu ra\n",
    "        with torch.no_grad():\n",
    "            output_shape = self.feature_extractor(sample_input).shape\n",
    "        num_feature_channels = output_shape[1]  # Số kênh của feature map\n",
    "\n",
    "        # Head layers cho object detection\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(num_feature_channels, num_feature_channels, kernel_size=3, stride=1, padding=1, groups=num_feature_channels),  # Depthwise\n",
    "            nn.Conv2d(num_feature_channels, 16, kernel_size=1, stride=1),  # Pointwise giảm xuống 16 kênh\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Đảm bảo đầu ra có kích thước [1, 3, 16, 16]\n",
    "        self.logits = nn.Conv2d(16, num_classes+1, kernel_size=1, stride=1)  # num_classes = 3 (3 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Chuyển từ 1 kênh sang 3 kênh bằng repeat (đúng chuẩn hơn)\n",
    "        x = x.repeat(1, 3, 1, 1)  \n",
    "\n",
    "        # Trích xuất đặc trưng\n",
    "        x = self.feature_extractor(x)\n",
    "        \n",
    "        # Áp dụng head layers\n",
    "        x = self.head(x)\n",
    "        \n",
    "        # Đảm bảo kích thước đầu ra là 16x16 thay vì 8x8\n",
    "        x = nn.functional.interpolate(x, size=(16, 16), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        x = self.logits(x)  # Logits output với shape (batch_size, 3, 16, 16)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43f0169-88b1-4e0c-88ab-7882fb18fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Đường dẫn đầy đủ đến pretrained weights\n",
    "pretrain_path = r\"notebook\\mobilenetv2.pytorch\\pretrained\\mobilenetv2_0.1-7d1d638a.pth\"\n",
    "\n",
    "model = FOMOWithMobileNetV2(input_channels=1, input_size=(128, 128), num_classes=2, pretrain_path=pretrain_path)\n",
    "model.to(device)\n",
    "summary(model, input_size=(1, 128, 128))  # Sửa input_size cho grayscale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1295f9ce-e260-43ae-a801-4f64c1d92dcf",
   "metadata": {},
   "source": [
    "Tạo Loss Function, Optimizer, Training\n",
    "Thiết lập thông số Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecefdb7d-a944-4dbd-8cf2-68aead0885d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "\n",
    "# Kiểm tra kích thước dataset\n",
    "print(\"Số mẫu trong train_dataset:\", len(train_dataset))\n",
    "if len(train_dataset) < batch_size:\n",
    "    batch_size = len(train_dataset)\n",
    "    print(f\"Batch size lớn hơn số mẫu, giảm batch_size xuống: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44876237-c895-4707-bbf3-1a89a4899336",
   "metadata": {},
   "source": [
    "Khởi tạo\n",
    "Tính class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b4a4d4-81fc-4815-b1d2-733d54d47bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def calculate_class_weights(dataset, num_classes=2, grid_size=16):\n",
    "    \"\"\"\n",
    "    Tính trọng số lớp dựa trên tần suất xuất hiện trong segmentation map.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset chứa images và targets (DrowsinessDetectionFOMODataset).\n",
    "        num_classes: Số lớp đối tượng (không tính background, mặc định 2).\n",
    "        grid_size: Kích thước lưới (mặc định 16).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Trọng số cho từng lớp (background, Drowsy, Non Drowsy).\n",
    "    \"\"\"\n",
    "    # Khởi tạo mảng đếm số ô cho mỗi lớp\n",
    "    class_counts = np.zeros(num_classes + 1)  # +1 cho background\n",
    "    total_pixels = 0\n",
    "\n",
    "    # Duyệt qua dataset\n",
    "    for _, target in dataset:\n",
    "        # target: (3, 16, 16) dạng one-hot\n",
    "        target_indices = torch.argmax(target, dim=0)  # Chuyển sang (16, 16) dạng class indices\n",
    "        # Đếm số ô cho mỗi lớp\n",
    "        for cls in range(num_classes + 1):\n",
    "            class_counts[cls] += (target_indices == cls).sum().item()\n",
    "        total_pixels += target_indices.numel()  # Tổng số ô (16 * 16)\n",
    "\n",
    "    # Tính trọng số: nghịch đảo tần suất chuẩn hóa\n",
    "    class_weights = total_pixels / (num_classes + 1) / class_counts\n",
    "    # Tránh chia cho 0 (nếu lớp nào không xuất hiện)\n",
    "    class_weights = np.where(class_counts > 0, class_weights, 1.0)\n",
    "    # Chuẩn hóa để trọng số không quá lớn\n",
    "    class_weights = class_weights / class_weights.min()\n",
    "\n",
    "    return torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# Tính class weights từ train_dataset\n",
    "#class_weights = calculate_class_weights(train_dataset, num_classes=2, grid_size=16)\n",
    "#print(\"Class weights (background, Drowsy, Non Drowsy):\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd778ac-c24f-46e7-add8-e19523a74ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Tính class weights\n",
    "class_weights = calculate_class_weights(train_dataset, num_classes=2, grid_size=16)\n",
    "print(\"Class weights (background, Drowsy, Non Drowsy):\", class_weights)\n",
    "class_weights = class_weights.to(device)  # [background, Drowsy, Non Drowsy]\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Khởi tạo optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Tạo DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Số batch trong train_loader:\", len(train_loader))\n",
    "print(\"Số batch trong val_loader:\", len(val_loader))\n",
    "print(\"Số batch trong test_loader:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581763be-bbaf-4b92-89ee-32ef5896a39e",
   "metadata": {},
   "source": [
    "Huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14928121-4176-4f28-a017-821f346bfde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "best_model_path = r\"model\\best_model_grayscale_128x128_24_05_2025.pth\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on {device}\")\n",
    "\n",
    "# Chuyển model sang device\n",
    "model = model.to(device)\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "best_val_epoch = 0\n",
    "epochs_no_improve = 0\n",
    "early_stop_patience = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        # Chuyển targets từ one-hot (batch_size, 3, 16, 16) sang class indices (batch_size, 16, 16)\n",
    "        targets_indices = torch.argmax(targets, dim=1)  # Lấy chỉ số lớp lớn nhất\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  # Shape: (batch_size, 3, 16, 16)\n",
    "\n",
    "        # Tính loss\n",
    "        loss = criterion(outputs, targets_indices)  # outputs: (batch_size, 3, 16, 16), targets_indices: (batch_size, 16, 16)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Tính accuracy\n",
    "        _, predicted = torch.max(outputs, 1)  # Shape: (batch_size, 16, 16)\n",
    "        correct_train += (predicted == targets_indices).sum().item()\n",
    "        total_train += targets_indices.numel()  # Tổng số ô grid (batch_size * 16 * 16)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Tính train loss và accuracy\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Đánh giá trên tập validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            targets_indices = torch.argmax(targets, dim=1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets_indices)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Tính accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == targets_indices).sum().item()\n",
    "            total_val += targets_indices.numel()\n",
    "\n",
    "    # Tính validation loss và accuracy\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Lưu mô hình tốt nhất\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(\"Validation loss tốt nhất, lưu model.\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # Early stopping\n",
    "    if epochs_no_improve >= early_stop_patience:\n",
    "        print(\"Early stopping: Validation loss không thay đổi thêm.\")\n",
    "        break\n",
    "\n",
    "# Tải mô hình tốt nhất\n",
    "print(f'Kết thúc huấn luyện. Load weights tốt nhất ở epoch {best_val_epoch}')\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb9b875-42ce-4c5c-afe4-895e2099d2a1",
   "metadata": {},
   "source": [
    "Vẽ đồ thị quá trình huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16fa38f-a39a-497a-af8d-2be18cce2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Vẽ đồ thị Loss và Accuracy\n",
    "# Vẽ đồ thị Loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', color='blue')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "# Vẽ đồ thị Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy', color='blue')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29297cbf-1f42-4a23-8eb4-29b51f9bfa03",
   "metadata": {},
   "source": [
    "Đánh giá mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d6184-1cc5-4b12-9122-a6c9951466fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đánh giá trên tập kiểm tra (test)\n",
    "all_lbs = []\n",
    "all_predictions = []\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for images, targets in test_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        targets_indices = torch.argmax(targets, dim=1)  # Chuyển sang class indices\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets_indices)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_lbs.extend(targets_indices.cpu().flatten().numpy())\n",
    "        all_predictions.extend(predicted.cpu().flatten().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9160ba8c-e22b-421b-97d3-d83a77fd72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tính test loss trung bình\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Tính ma trận nhầm lẫn\n",
    "cm = confusion_matrix(all_lbs, all_predictions, labels=[0, 1, 2])\n",
    "\n",
    "# Hiển thị ma trận nhầm lẫn\n",
    "plt.figure(figsize=(8, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['background', 'Drowsy', 'Non Drowsy'])\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "precision = precision_score(all_lbs, all_predictions, average='weighted')\n",
    "recall = recall_score(all_lbs, all_predictions, average='weighted')\n",
    "f1 = f1_score(all_lbs, all_predictions, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# In báo cáo chi tiết\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_lbs, all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ce5f2-3b56-42ea-b6d7-3237747e1afc",
   "metadata": {},
   "source": [
    "Chạy thử model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67734dd-4bd5-4a71-a1dd-3e4d4c6a44d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),   # Chuyển PIL Image thành tensor (shape: 1, 128, 128)\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Chuẩn hóa cho ảnh grayscale\n",
    "])\n",
    "\n",
    "# Chạy thử mô hình trên một ảnh test\n",
    "idx = random.randint(0, len(X_test) - 1)\n",
    "#idx = 6  # Gán cứng để kiểm tra\n",
    "image = X_test[idx]\n",
    "\n",
    "model.eval()\n",
    "# Chuẩn bị đầu vào\n",
    "image_pil = Image.fromarray((image.squeeze() * 255).astype(np.uint8), mode='L')\n",
    "x = transform(image_pil).unsqueeze(0).to(device)  # Shape: (1, 1, 128, 128)\n",
    "y = bbox_to_segmentation(image.shape, y_test[idx], 16, num_classes=2).unsqueeze(0).to(device)  # Shape: (1, 3, 16, 16)\n",
    "y_target = torch.argmax(y, dim=1)  # Shape: (1, 16, 16)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(x)  # Shape: (1, 3, 16, 16)\n",
    "    _, y_predicted = torch.max(output, 1)  # Shape: (1, 16, 16)\n",
    "\n",
    "# Hiển thị kết quả\n",
    "print(f\"Target segmentation map (first few values): {y_target[::]}\")\n",
    "print(f\"Predicted segmentation map (first few values): {y_predicted[::]}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Ảnh gốc với tâm dự đoán\n",
    "plt.subplot(1, 2, 1)\n",
    "centers_pred = export_center(y_predicted[0], image.shape)\n",
    "plt.imshow(draw_center(image, centers_pred))\n",
    "plt.title(f\"Predicted (Idx: {idx})\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Ảnh gốc với tâm ground truth\n",
    "plt.subplot(1, 2, 2)\n",
    "centers_target = export_center(y_target[0], image.shape)\n",
    "plt.imshow(draw_center(image, centers_target))\n",
    "plt.title(f\"Ground Truth (Idx: {idx})\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f3fa6-236f-414c-8b76-7fb93e8a1412",
   "metadata": {},
   "source": [
    "Chuyển đổi sang OnnxRuntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53726ad-6f02-416f-9ea7-cf7e0128c5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Đặt mô hình ở chế độ đánh giá\n",
    "model.eval()\n",
    "\n",
    "# Dummy input (kích thước phải giống với kích thước đầu vào của mô hình)\n",
    "# Ví dụ: Input [batch_size=1, channels=3, height=224, width=224]\n",
    "dummy_input = torch.randn(1, 1, 128, 128).to(device)  # Đầu vào grayscale\n",
    "\n",
    "# Đường dẫn lưu mô hình ONNX\n",
    "onnx_file_path = r\"model\\best_model_grayscale_128x128_24_05_2025.onnx\"\n",
    "\n",
    "# Xuất mô hình sang định dạng ONNX\n",
    "torch.onnx.export(\n",
    "    model,                        # Mô hình PyTorch\n",
    "    dummy_input,                  # Dummy input\n",
    "    onnx_file_path,               # Đường dẫn lưu file .onnx\n",
    "    export_params=True,           # Lưu tất cả tham số (weights) vào ONNX\n",
    "    opset_version=12,             # Phiên bản ONNX (tùy chỉnh, thường dùng 11 hoặc 12)\n",
    "    do_constant_folding=True,     # Tối ưu hóa các hằng số trong mô hình\n",
    "    input_names=[\"input\"],        # Tên đầu vào (có thể đặt tùy ý)\n",
    "    output_names=[\"output\"],      # Tên đầu ra (có thể đặt tùy ý)\n",
    "    dynamic_axes={                # Định nghĩa trục động (batch size)\n",
    "        \"input\": {0: \"batch_size\"},\n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Model has been converted to ONNX and saved at {onnx_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
